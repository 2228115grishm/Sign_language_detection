{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# blank comment and added on more comment\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.DATA COLLECTION UTPTO 1000 images per label\n",
    "#### here we detct the input from users camera and save the images in the respective folder\n",
    "#### also the part of image containing hand is saved by cropping it \n",
    "#### the only problem in this is with gestures requiring two hands\n",
    "#### to solve this issue a diffrent approach need to be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 's' to save the cropped hand image with label, or 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "def cptr_crop(nm_img):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    \n",
    "    fldName = \"CollectedData\"\n",
    "    if not os.path.exists(fldName):\n",
    "        os.makedirs(fldName)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"Press 's' to save the cropped hand image with label, or 'q' to quit.\")\n",
    "    \n",
    "    img_cnt = 0\n",
    "    \n",
    "    while img_cnt < nm_img:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture image\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rslt = hands.process(img_rgb)\n",
    "        \n",
    "    \n",
    "        if rslt.multi_hand_landmarks:\n",
    "            for hand_landmarks in rslt.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        \n",
    "        cv2.imshow(\"Capture Image with Landmarks\", frame)\n",
    "        #caputerr the imae with label and landmarkkk\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('s') and img_cnt < nm_img:  \n",
    "        \n",
    "            root = tk.Tk()\n",
    "            root.withdraw()\n",
    "            label = simpledialog.askstring(\"Input\", \"Enter the label for this image:\")\n",
    "            root.destroy()\n",
    "            \n",
    "            \n",
    "            # Take lanbel inpute from the user ....\n",
    "            #The only problem is that for a single label user had to enter same text multiple time \n",
    "            # problem need to be considered i think it can be resolved using the previously used GUI interface ... handeled later\n",
    "            \n",
    "            \n",
    "            if label and rslt.multi_hand_landmarks:\n",
    "                \n",
    "                label_dir = os.path.join(fldName, label)\n",
    "                \n",
    "                if not os.path.exists(label_dir):\n",
    "                    os.makedirs(label_dir)\n",
    "                \n",
    "                for idx, hand_landmarks in enumerate(rslt.multi_hand_landmarks):\n",
    "                    \n",
    "                    img_h, img_w, _ = frame.shape\n",
    "                    x_min, y_min = img_w, img_h\n",
    "                    x_max, y_max = 0, 0\n",
    "                    \n",
    "                    for landmark in hand_landmarks.landmark:\n",
    "                        x, y = int(landmark.x * img_w), int(landmark.y * img_h)\n",
    "                        x_min, y_min = min(x, x_min), min(y, y_min)\n",
    "                        x_max, y_max = max(x, x_max), max(y, y_max)\n",
    "                    \n",
    "                    \n",
    "                    padding = 20\n",
    "                    x_min = max(0, x_min - padding)\n",
    "                    y_min = max(0, y_min - padding)\n",
    "                    x_max = min(img_w, x_max + padding)\n",
    "                    y_max = min(img_h, y_max + padding)\n",
    "                    \n",
    "                    \n",
    "                    cropped_hand = frame[y_min:y_max, x_min:x_max]\n",
    "                    \n",
    "                    \n",
    "                    file_count = len(os.listdir(label_dir)) + 1 \n",
    "                    file_name = os.path.join(label_dir, f\"{file_count}.jpg\")\n",
    "                    cv2.imwrite(file_name, cropped_hand)\n",
    "                    print(f\"Cropped hand image saved as {file_name}\")\n",
    "                    img_cnt += 1\n",
    "                    \n",
    "                    \n",
    "                    if img_cnt >= nm_img:\n",
    "                        break\n",
    "            else:\n",
    "                print(\"No hand detected or label not provided.\")\n",
    "        \n",
    "        elif key == ord('q'):  \n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "cptr_crop(nm_img=100) \n",
    "# The only isssue with this part was saving multiple images with same label was not possible \n",
    "# so the thought was to make separate folder for each label and insert a multiple images in that folder \n",
    "# the labels will be later handeled using label encoder or any other technique \n",
    "\n",
    "# Code runnuhgg fine with no errors debugging not done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading the data and using label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fldName):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in os.listdir(fldName):\n",
    "        label_dir = os.path.join(fldName, label)\n",
    "        for img_file in os.listdir(label_dir):\n",
    "            img_path = os.path.join(label_dir, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Image at {img_path} is not valid and will be skipped.\")\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
    "            img = cv2.resize(img, (128, 128)) \n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    \n",
    "    images = np.array(images) / 255.0 \n",
    "    labels = np.array(labels)\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    np.save('label_class.npy', label_encoder.classes_)\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# LAbel encoder seems fine for now no error saved label of encoder so that we can revert back them to respective labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. CNN model (model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 333ms/step - accuracy: 0.6363 - loss: 1.4600 - val_accuracy: 0.9694 - val_loss: 0.1140\n",
      "Epoch 2/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 312ms/step - accuracy: 0.9749 - loss: 0.1071 - val_accuracy: 0.9774 - val_loss: 0.0780\n",
      "Epoch 3/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 325ms/step - accuracy: 0.9913 - loss: 0.0416 - val_accuracy: 0.9920 - val_loss: 0.0591\n",
      "Epoch 4/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 379ms/step - accuracy: 0.9937 - loss: 0.0164 - val_accuracy: 0.9880 - val_loss: 0.0720\n",
      "Epoch 5/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 372ms/step - accuracy: 0.9984 - loss: 0.0044 - val_accuracy: 0.9907 - val_loss: 0.0610\n",
      "Epoch 6/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 327ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 0.9893 - val_loss: 0.0970\n",
      "Epoch 7/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 313ms/step - accuracy: 0.9977 - loss: 0.0206 - val_accuracy: 0.9854 - val_loss: 0.0702\n",
      "Epoch 8/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 321ms/step - accuracy: 0.9979 - loss: 0.0060 - val_accuracy: 0.9867 - val_loss: 0.0685\n",
      "Epoch 9/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 320ms/step - accuracy: 0.9993 - loss: 0.0040 - val_accuracy: 0.9880 - val_loss: 0.0645\n",
      "Epoch 10/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 314ms/step - accuracy: 0.9991 - loss: 0.0040 - val_accuracy: 0.9880 - val_loss: 0.0561\n",
      "Epoch 11/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 319ms/step - accuracy: 0.9986 - loss: 0.0053 - val_accuracy: 0.9893 - val_loss: 0.0689\n",
      "Epoch 12/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 316ms/step - accuracy: 1.0000 - loss: 2.7382e-04 - val_accuracy: 0.9893 - val_loss: 0.0736\n",
      "Epoch 13/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 315ms/step - accuracy: 1.0000 - loss: 4.6981e-05 - val_accuracy: 0.9893 - val_loss: 0.0747\n",
      "Epoch 14/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 319ms/step - accuracy: 1.0000 - loss: 2.9436e-05 - val_accuracy: 0.9893 - val_loss: 0.0765\n",
      "Epoch 15/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 324ms/step - accuracy: 1.0000 - loss: 2.0391e-05 - val_accuracy: 0.9920 - val_loss: 0.0772\n",
      "Epoch 16/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 317ms/step - accuracy: 1.0000 - loss: 1.3864e-05 - val_accuracy: 0.9920 - val_loss: 0.0776\n",
      "Epoch 17/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 314ms/step - accuracy: 1.0000 - loss: 1.0902e-05 - val_accuracy: 0.9920 - val_loss: 0.0786\n",
      "Epoch 18/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 315ms/step - accuracy: 1.0000 - loss: 9.3652e-06 - val_accuracy: 0.9920 - val_loss: 0.0793\n",
      "Epoch 19/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 318ms/step - accuracy: 1.0000 - loss: 5.5548e-06 - val_accuracy: 0.9920 - val_loss: 0.0803\n",
      "Epoch 20/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 321ms/step - accuracy: 1.0000 - loss: 6.7840e-06 - val_accuracy: 0.9920 - val_loss: 0.0813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/model1.h5\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(len(np.unique(y_train)), activation='softmax')  # Number of output classes\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "fldName = \"CollectedData\"\n",
    "x_train, x_test, y_train, y_test = load_data(fldName)\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)\n",
    "model_save_path = 'models/model1.h5'\n",
    "\n",
    "\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 CNN 2 model(model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 319ms/step - accuracy: 0.6157 - loss: 1.4463 - val_accuracy: 0.9800 - val_loss: 0.0972\n",
      "Epoch 2/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 311ms/step - accuracy: 0.9869 - loss: 0.0636 - val_accuracy: 0.9800 - val_loss: 0.0904\n",
      "Epoch 3/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 310ms/step - accuracy: 0.9883 - loss: 0.0493 - val_accuracy: 0.9867 - val_loss: 0.0879\n",
      "Epoch 4/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 311ms/step - accuracy: 0.9935 - loss: 0.0229 - val_accuracy: 0.9787 - val_loss: 0.0857\n",
      "Epoch 5/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 305ms/step - accuracy: 0.9950 - loss: 0.0139 - val_accuracy: 0.9920 - val_loss: 0.0727\n",
      "Epoch 6/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 306ms/step - accuracy: 0.9979 - loss: 0.0055 - val_accuracy: 0.9867 - val_loss: 0.0942\n",
      "Epoch 7/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 305ms/step - accuracy: 0.9980 - loss: 0.0087 - val_accuracy: 0.9867 - val_loss: 0.0824\n",
      "Epoch 8/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 306ms/step - accuracy: 0.9988 - loss: 0.0067 - val_accuracy: 0.9880 - val_loss: 0.0838\n",
      "Epoch 9/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 312ms/step - accuracy: 0.9986 - loss: 0.0071 - val_accuracy: 0.9840 - val_loss: 0.0796\n",
      "Epoch 10/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 310ms/step - accuracy: 1.0000 - loss: 7.6760e-05 - val_accuracy: 0.9854 - val_loss: 0.0829\n",
      "Epoch 11/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 307ms/step - accuracy: 1.0000 - loss: 3.0324e-05 - val_accuracy: 0.9854 - val_loss: 0.0843\n",
      "Epoch 12/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 3.1262e-05 - val_accuracy: 0.9854 - val_loss: 0.0854\n",
      "Epoch 13/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 1.5191e-05 - val_accuracy: 0.9854 - val_loss: 0.0866\n",
      "Epoch 14/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 1.2278e-05 - val_accuracy: 0.9854 - val_loss: 0.0875\n",
      "Epoch 15/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 1.2183e-05 - val_accuracy: 0.9854 - val_loss: 0.0884\n",
      "Epoch 16/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 307ms/step - accuracy: 1.0000 - loss: 8.6975e-06 - val_accuracy: 0.9854 - val_loss: 0.0893\n",
      "Epoch 17/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 310ms/step - accuracy: 1.0000 - loss: 8.0652e-06 - val_accuracy: 0.9880 - val_loss: 0.0900\n",
      "Epoch 18/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 310ms/step - accuracy: 1.0000 - loss: 5.9486e-06 - val_accuracy: 0.9880 - val_loss: 0.0907\n",
      "Epoch 19/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 311ms/step - accuracy: 1.0000 - loss: 5.5276e-06 - val_accuracy: 0.9880 - val_loss: 0.0914\n",
      "Epoch 20/20\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 307ms/step - accuracy: 1.0000 - loss: 7.7150e-06 - val_accuracy: 0.9880 - val_loss: 0.0920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/model2.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "fldName = \"CollectedData\"\n",
    "x_train, x_test, y_train, y_test = load_data(fldName)\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)\n",
    "model_save_path = 'models/model2.h5'\n",
    "\n",
    "\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 MobileNetV2(model 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\model3.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(len(np.unique(y_train)), activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False  # Freeze base layers\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "fldName = \"CollectedData\"\n",
    "x_train, x_test, y_train, y_test = load_data(fldName)\n",
    "\n",
    "models_folder = \"models\"\n",
    "if not os.path.exists(models_folder):\n",
    "    os.makedirs(models_folder)\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(models_folder, \"model3.h5\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PREDICTION REAL TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 55 (1226743318.py, line 56)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 56\u001b[1;36m\u001b[0m\n\u001b[1;33m    cap = cv2.VideoCapture(0)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 55\n"
     ]
    }
   ],
   "source": [
    "model = load_model('models/model2.h5')\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('label_class.npy')  \n",
    "\n",
    "# Loading the saved labels and saved model\n",
    "# or else we would have to train it every time we use it \n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def prdct_gstr():\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"Press 'q' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture image\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rslt = hands.process(img_rgb)\n",
    "        \n",
    "        if rslt.multi_hand_landmarks:\n",
    "            for hand_landmarks in rslt.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                \n",
    "                img_h, img_w, _ = frame.shape\n",
    "                x_min, y_min = img_w, img_h\n",
    "                x_max, y_max = 0, 0\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * img_w), int(landmark.y * img_h)\n",
    "                    x_min, y_min = min(x, x_min), min(y, y_min)\n",
    "                    x_max, y_max = max(x, x_max), max(y, y_max)\n",
    "\n",
    "                padding = 20\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(img_w, x_max + padding)\n",
    "                y_max = min(img_h, y_max + padding)\n",
    "                \n",
    "                cropped_hand = frame[y_min:y_max, x_min:x_max]\n",
    "                cropped_hand = cv2.cvtColor(cropped_hand, cv2.COLOR_BGR2RGB) \n",
    "                cropped_hand = cv2.resize(cropped_hand, (128, 128))  \n",
    "                cropped_hand = np.expand_dims(cropped_hand, axis=0) / 255.0  \n",
    "                \n",
    "            \n",
    "                predictions = model.predict(cropped_hand)\n",
    "                gesture_label_encoded = np.argmax(predictions, axis=1)[0]\n",
    "                gesture_label = label_encoder.inverse_transform([gesture_label_encoded])[0]\n",
    "                \n",
    "                def prdct_gstr():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"Press 'q' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture image\")\n",
    "           \n",
    "                cv2.putText(frame, f'Gesture: {gesture_label}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    \n",
    "        cv2.imshow(\"Real-time Hand Gesture Prediction\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):  # Quit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "prdct_gstr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
